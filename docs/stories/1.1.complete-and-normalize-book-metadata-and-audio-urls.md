# Story 1.1: Complete and normalize book metadata and audio URLs

## Status

Draft

## Story

**As an** operator,  
**I want** the crawler to extract full book metadata and valid audio URLs for each book,  
**so that** the database-viewer and GrqaserApp have complete, playable catalog data.

## Acceptance Criteria

1. Duration is parsed into a structured format (e.g., hours, minutes, formatted string).
2. Audio URLs are extracted from book detail pages, validated, and stored.
3. Categories/genres, ratings, descriptions, and language are extracted and stored per book.
4. Output schema is documented and consistent (e.g., unique IDs, required fields).

## Tasks / Subtasks

- [ ] Task 1: Duration parsing and storage (AC: 1)
  - [ ] Add or reuse a duration parser that converts source-site duration (e.g. "0ժ 51ր" or similar) into a structured format (hours, minutes) and/or a normalized formatted string.
  - [ ] Persist duration in the books table in a way that supports both structured use and display (align with [Source: docs/architecture/data-models-and-schema.md]).
  - [ ] Unit test duration parsing and normalization.
- [ ] Task 2: Audio URL extraction and validation (AC: 2)
  - [ ] Extract audio URLs from book detail pages (Puppeteer or existing crawler flow).
  - [ ] Validate URLs (e.g. reachable, correct scheme) and store only valid ones; log or skip invalid.
  - [ ] Store main_audio_url (and download_url if applicable) per book in the schema.
  - [ ] Unit test URL validation logic; integration test that valid URLs are persisted.
- [ ] Task 3: Categories, ratings, descriptions, language (AC: 3)
  - [ ] Extract categories/genres, ratings, descriptions, and language from book detail (and listing where relevant).
  - [ ] Clean text fields: no HTML, normalized encoding ([Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
  - [ ] Store in books table (or linked tables per data model); ensure relationships support filtering/stats later.
  - [ ] Unit test parsing and cleaning; integration test DB writes for these fields.
- [ ] Task 4: Schema consistency and documentation (AC: 4)
  - [ ] Ensure books table has unique IDs and required fields; validate types before write; log/skip invalid rows ([Source: docs/architecture/coding-standards.md], [Source: docs/architecture/data-models-and-schema.md]).
  - [ ] Document the current schema (tables, columns, constraints) in docs/architecture/data-models-and-schema.md or a dedicated schema doc referenced there.
  - [ ] Add or update integration tests that assert schema and required-field behavior.

## Dev Notes

### Previous Story Insights

First story in Epic 1; no prior story. Follow Phase 1 only—no database-viewer or GrqaserApp dependencies.

### Data Models

- **Books:** id, title, author, description, duration (structured e.g. hours/minutes and/or formatted string), type, language, category, rating, cover_image_url, main_audio_url, download_url, crawl_status, has_chapters, chapter URLs where applicable. No HTML in text fields; consistent encoding; unique IDs ([Source: docs/architecture/data-models-and-schema.md]). Duration shape: structured (hours, minutes) and/or formatted string (e.g. "0ժ 51ր") per PRD/Story 1.1.
- **Existing crawler DB:** `crawler/src/models/database.js` defines `books` (and url_queue, crawl_logs). Align new/updated columns with the shared data contract; schema ownership is Phase 1 and must be documented.

### API Specifications

N/A for this story (crawler-only; no REST API).

### File Locations

- **Crawler entry:** `crawler/src/crawler.js` (or main script in use).
- **Config:** `crawler/src/config/crawler-config.js` (DB path, timeouts, retries, logging).
- **DB layer:** `crawler/src/models/database.js` (createTables, book inserts/updates).
- **URL queue:** `crawler/src/utils/url-queue-manager.js` (if detail URLs are queued).
- **DB file:** `crawler/data/grqaser.db` ([Source: docs/architecture/source-tree.md], [Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
- **Schema documentation:** Update or extend `docs/architecture/data-models-and-schema.md` (or linked schema doc) with tables, columns, constraints.

### Technical Constraints

- Node.js LTS; Puppeteer for browser automation; SQLite 3.x; Jest for tests ([Source: docs/architecture/tech-stack.md]).
- Timeouts and retries (e.g. 30s timeout, 3 retries); log errors ([Source: docs/architecture/coding-standards.md]).
- Only the crawler writes to the canonical SQLite DB; no HTML in persisted text fields; validate required fields and types before write ([Source: docs/architecture/coding-standards.md]).

### Project Structure Notes

New crawler code lives under `crawler/src/`; respect existing config and models ([Source: docs/architecture/source-tree.md]).

### Testing

- **Unit tests:** Parsing and normalization (duration, URL validation, text cleaning). No HTML in text fields; required fields and types ([Source: docs/architecture/testing-and-deployment-strategy.md], [Source: docs/architecture/coding-standards.md]). Place tests next to source or in `crawler/` test layout (e.g. `__tests__/` or `*.test.js`).
- **Integration tests:** DB writes; schema and constraints; optional smoke against a test page ([Source: docs/architecture/testing-and-deployment-strategy.md]).
- **Run:** `npm test` from `crawler/` ([Source: docs/architecture/tech-stack.md]).

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 1     | Scrum Master |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

—

### Completion Notes List

—

### File List

—

## QA Results

—

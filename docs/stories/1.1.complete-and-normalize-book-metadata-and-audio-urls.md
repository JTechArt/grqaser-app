# Story 1.1: Complete and normalize book metadata and audio URLs

## Status

Done

## Story

**As an** operator,  
**I want** the crawler to extract full book metadata and valid audio URLs for each book,  
**so that** the database-viewer and GrqaserApp have complete, playable catalog data.

## Acceptance Criteria

1. Duration is parsed into a structured format (e.g., hours, minutes, formatted string).
2. Audio URLs are extracted from book detail pages, validated, and stored.
3. Categories/genres, ratings, descriptions, and language are extracted and stored per book.
4. Output schema is documented and consistent (e.g., unique IDs, required fields).

## Tasks / Subtasks

- [x] Task 1: Duration parsing and storage (AC: 1)
  - [x] Add or reuse a duration parser that converts source-site duration (e.g. "0ժ 51ր" or similar) into a structured format (hours, minutes) and/or a normalized formatted string.
  - [x] Persist duration in the books table in a way that supports both structured use and display (align with [Source: docs/architecture/data-models-and-schema.md]).
  - [x] Unit test duration parsing and normalization.
- [x] Task 2: Audio URL extraction and validation (AC: 2)
  - [x] Extract audio URLs from book detail pages (Puppeteer or existing crawler flow).
  - [x] Validate URLs (e.g. reachable, correct scheme) and store only valid ones; log or skip invalid.
  - [x] Store main_audio_url (and download_url if applicable) per book in the schema.
  - [x] Unit test URL validation logic; integration test that valid URLs are persisted.
- [x] Task 3: Categories, ratings, descriptions, language (AC: 3)
  - [x] Extract categories/genres, ratings, descriptions, and language from book detail (and listing where relevant).
  - [x] Clean text fields: no HTML, normalized encoding ([Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
  - [x] Store in books table (or linked tables per data model); ensure relationships support filtering/stats later.
  - [x] Unit test parsing and cleaning; integration test DB writes for these fields.
- [x] Task 4: Schema consistency and documentation (AC: 4)
  - [x] Ensure books table has unique IDs and required fields; validate types before write; log/skip invalid rows ([Source: docs/architecture/coding-standards.md], [Source: docs/architecture/data-models-and-schema.md]).
  - [x] Document the current schema (tables, columns, constraints) in docs/architecture/data-models-and-schema.md or a dedicated schema doc referenced there.
  - [x] Add or update integration tests that assert schema and required-field behavior.

## Dev Notes

### Previous Story Insights

First story in Epic 1; no prior story. Follow Phase 1 only—no database-viewer or GrqaserApp dependencies.

### Data Models

- **Books:** id, title, author, description, duration (structured e.g. hours/minutes and/or formatted string), type, language, category, rating, cover_image_url, main_audio_url, download_url, crawl_status, has_chapters, chapter URLs where applicable. No HTML in text fields; consistent encoding; unique IDs ([Source: docs/architecture/data-models-and-schema.md]). Duration shape: structured (hours, minutes) and/or formatted string (e.g. "0ժ 51ր") per PRD/Story 1.1.
- **Existing crawler DB:** `crawler/src/models/database.js` defines `books` (and url_queue, crawl_logs). Align new/updated columns with the shared data contract; schema ownership is Phase 1 and must be documented.

### API Specifications

N/A for this story (crawler-only; no REST API).

### File Locations

- **Crawler entry:** `crawler/src/crawler.js` (or main script in use).
- **Config:** `crawler/src/config/crawler-config.js` (DB path, timeouts, retries, logging).
- **DB layer:** `crawler/src/models/database.js` (createTables, book inserts/updates).
- **URL queue:** `crawler/src/utils/url-queue-manager.js` (if detail URLs are queued).
- **DB file:** `crawler/data/grqaser.db` ([Source: docs/architecture/source-tree.md], [Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
- **Schema documentation:** Update or extend `docs/architecture/data-models-and-schema.md` (or linked schema doc) with tables, columns, constraints.

### Technical Constraints

- Node.js LTS; Puppeteer for browser automation; SQLite 3.x; Jest for tests ([Source: docs/architecture/tech-stack.md]).
- Timeouts and retries (e.g. 30s timeout, 3 retries); log errors ([Source: docs/architecture/coding-standards.md]).
- Only the crawler writes to the canonical SQLite DB; no HTML in persisted text fields; validate required fields and types before write ([Source: docs/architecture/coding-standards.md]).

### Project Structure Notes

New crawler code lives under `crawler/src/`; respect existing config and models ([Source: docs/architecture/source-tree.md]).

### Testing

- **Unit tests:** Parsing and normalization (duration, URL validation, text cleaning). No HTML in text fields; required fields and types ([Source: docs/architecture/testing-and-deployment-strategy.md], [Source: docs/architecture/coding-standards.md]). Place tests next to source or in `crawler/` test layout (e.g. `__tests__/` or `*.test.js`).
- **Integration tests:** DB writes; schema and constraints; optional smoke against a test page ([Source: docs/architecture/testing-and-deployment-strategy.md]).
- **Run:** `npm test` from `crawler/` ([Source: docs/architecture/tech-stack.md]).

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 1     | Scrum Master |
| 2025-02-14 | 1.1     | Story 1.1 implemented: duration parser, URL validator, text cleaner, book validator; schema doc; crawler integration; tests | Dev Agent |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

- `npm test` (crawler): 36 tests passed (duration-parser, url-validator, text-cleaner, book-validator, database.integration). Puppeteer test excluded via jest.config.js.

### Completion Notes List

- Task 1: Added `crawler/src/utils/duration-parser.js` (parseDuration, normalizeDurationForStorage); books table now has `duration_formatted`; migration in database.js for existing DBs; unit tests in duration-parser.test.js.
- Task 2: Added `crawler/src/utils/url-validator.js` (validateAudioUrl, filterValidUrls); extractBookDetail validates main_audio_url and download_url, stores only valid; unit tests url-validator.test.js; integration test in database.integration.test.js for persisted URLs.
- Task 3: Added `crawler/src/utils/text-cleaner.js` (cleanText); extractBookDetail extracts category, language, rating, description and cleans all text; unit tests text-cleaner.test.js; DB integration test covers full book persist.
- Task 4: Added `crawler/src/utils/book-validator.js` (validateBookRow); saveBooks validates before write and skips invalid with log; schema documented in docs/architecture/data-models-and-schema.md (Books table DDL); integration tests assert schema columns and required-field behavior. Jest config excludes crawler.test.js so `npm test` runs without Chrome.

### File List

- crawler/src/utils/duration-parser.js (new)
- crawler/src/utils/url-validator.js (new)
- crawler/src/utils/text-cleaner.js (new)
- crawler/src/utils/book-validator.js (new)
- crawler/src/models/database.js (modified: duration_formatted column, migration, saveBook params)
- crawler/src/crawler.js (modified: config, createBooksTableIfNeeded, extractBookDetail normalization, saveBooks validation, saveBookToDatabase full schema)
- crawler/src/tests/duration-parser.test.js (new)
- crawler/src/tests/url-validator.test.js (new)
- crawler/src/tests/text-cleaner.test.js (new)
- crawler/src/tests/book-validator.test.js (new)
- crawler/src/tests/database.integration.test.js (new)
- crawler/jest.config.js (new: exclude crawler.test.js)
- crawler/package.json (modified: test:unit script)
- docs/architecture/data-models-and-schema.md (modified: Books table DDL and required-fields note)

—

## QA Results

### Review Date: 2025-02-14

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Implementation is clear and well-structured. Four focused utility modules (duration-parser, url-validator, text-cleaner, book-validator) are used consistently by the crawler. Validation and normalization are applied before DB writes; invalid rows are skipped with logging. Schema is documented in `docs/architecture/data-models-and-schema.md` and matches the code (Books table DDL, required fields, no-HTML rule).

### Refactoring Performed

None. No blocking issues warranted code changes during review.

### Compliance Check

- Coding Standards: ✓ Config used; no HTML in persisted text; required fields and types validated before write; descriptive naming (camelCase/PascalCase).
- Project Structure: ✓ New code under `crawler/src/`; tests under `crawler/src/tests/`; schema doc updated.
- Testing Strategy: ✓ Unit tests for parsing/normalization/validation; integration tests for DB and schema; `npm test` runs 36 tests (Puppeteer E2E excluded via jest.config.js).
- All ACs Met: ✓ AC1 duration parsing/storage; AC2 audio URL extraction/validation/storage; AC3 categories, ratings, descriptions, language with text cleaning; AC4 schema consistency and documentation.

### Improvements Checklist

- [x] Requirements traceability verified (all four ACs have unit and/or integration coverage).
- [x] Test run confirmed (36 tests passed).
- [ ] Consider consolidating books table DDL in a single place (currently in both `database.js` and `crawler.js` createBooksTableIfNeeded) in a future story to avoid drift.

### Security Review

No auth or sensitive input in crawler scope. URL validation restricts schemes to http/https, reducing risk of javascript: or other non-http URLs being stored.

### Performance Considerations

Timeouts and retries are config-driven. Validation and text cleaning are synchronous and lightweight; acceptable for current crawler scale.

### Files Modified During Review

None.

### Gate Status

Gate: PASS → docs/qa/gates/1.1-complete-and-normalize-book-metadata-and-audio-urls.yml

### Recommended Status

✓ Ready for Done

---

### Review Date: 2025-02-14 (re-review)

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

No change to scope. Optional follow-up from initial review has been completed: books table DDL is now consolidated in `crawler/src/schema/books-table.js` and required by both `crawler.js` (createBooksTableIfNeeded) and `database.js` (createBooksTable), eliminating schema drift risk.

### Refactoring Performed

None during this re-review.

### Compliance Check

- Coding Standards: ✓ (unchanged)
- Project Structure: ✓ Schema module added under `crawler/src/schema/`.
- Testing Strategy: ✓ 36 tests passed.
- All ACs Met: ✓ (unchanged)

### Improvements Checklist

- [x] Requirements traceability verified (unchanged).
- [x] Test run confirmed (36 tests passed).
- [x] Books table DDL consolidated in `crawler/src/schema/books-table.js` (optional follow-up done).

### Security Review

No change.

### Performance Considerations

No change.

### Files Modified During Review

None.

### Gate Status

Gate: PASS → docs/qa/gates/1.1-complete-and-normalize-book-metadata-and-audio-urls.yml

### Recommended Status

✓ Ready for Done

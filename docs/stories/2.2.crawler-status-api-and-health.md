# Story 2.2: Crawler status API and health

## Status

Draft

## Story

**As an** operator,  
**I want** endpoints for crawler status, URL queue, and logs, plus a health check,  
**so that** I can monitor the pipeline from the admin panel or scripts.

## Acceptance Criteria

1. Crawler status (and optionally URL queue and logs) are exposed via API (e.g., `/api/v1/crawler/status`, `/urls`, `/logs`).
2. `GET /api/v1/health` returns service health and optionally DB connectivity.
3. Configuration (port, DB path, CORS, rate limit, logging) is externalized (e.g., env or config file).

## Tasks / Subtasks

- [ ] Task 1: Crawler status and queue/logs API (AC: 1)
  - [ ] Expose crawler status (and optionally URL queue and logs) via API, e.g. `/api/v1/crawler/status`, `/api/v1/crawler/urls`, `/api/v1/crawler/logs` ([Source: docs/architecture/database-viewer-api-and-deployment.md]).
  - [ ] Read from crawler-produced tables (url_queue, crawl_logs) or log files; read-only.
  - [ ] Unit/integration tests for crawler routes.
- [ ] Task 2: Health endpoint (AC: 2)
  - [ ] Implement `GET /api/v1/health` returning service health and DB connectivity ([Source: docs/architecture/database-viewer-api-and-deployment.md], [Source: docs/architecture/coding-standards.md]).
  - [ ] Test health returns 200 when DB is reachable; appropriate status when not.
- [ ] Task 3: Configuration externalization (AC: 3)
  - [ ] Externalize port, DB path, CORS, rate limit, logging (env or config file, e.g. `database-viewer/src/config/config.js`) ([Source: docs/architecture/database-viewer-api-and-deployment.md], [Source: docs/architecture/coding-standards.md]).
  - [ ] Document or example env; no hardcoded paths or secrets.

## Dev Notes

### Previous Story Insights

Story 2.1 delivers books and stats API. This story adds crawler monitoring and health; config must be externalized for deployment.

### Data Models

- **URL queue** (crawler): url, url_type, priority, status, retry_count, etc. **Crawl logs:** level, message, book_id, url, error_details ([Source: docs/architecture/data-models-and-schema.md]). Exposed read-only by database-viewer.

### API Specifications

- **Crawler:** `/api/v1/crawler/status`, `/urls`, `/logs`. **Health:** `GET /api/v1/health` ([Source: docs/architecture/database-viewer-api-and-deployment.md]).

### File Locations

- **Entry:** `database-viewer/src/server.js`. **Config:** `database-viewer/src/config/config.js`. **Routes:** e.g. `database-viewer/src/routes/crawler.js`. **Models:** `database-viewer/src/models/database.js` ([Source: docs/architecture/database-viewer-api-and-deployment.md]).

### Technical Constraints

- Config over hardcoding; health must reflect DB connectivity ([Source: docs/architecture/coding-standards.md]).

### Testing

- Unit tests for crawler and health handlers; integration tests with DB ([Source: docs/architecture/testing-and-deployment-strategy.md]). Run: `npm test` from `database-viewer/`.

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 2     | Scrum Master |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

—

### Completion Notes List

—

### File List

—

## QA Results

—

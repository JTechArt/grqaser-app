# Story 6.3: Crawler start/stop and config management

## Status

Draft

## Story

**As an** operator,  
**I want** to start and stop the data crawler from the books-admin-app and manage crawler configuration (including DB path/version) in the app,  
**so that** I can run and control the pipeline and adjust settings without editing config files by hand.

## Acceptance Criteria

1. User can **start** and **stop** the crawler from the app (web UI and/or API).
2. Crawler configuration (e.g. mode, DB path, rate limits, timeouts) is **manageable within the app** (view and edit); changes apply to subsequent crawler runs.
3. Crawler config includes or references the **active DB path** (aligned with Story 6.2) so that runs write to the correct db.{version}.
4. Crawler status and logs remain visible in the app (building on existing database-viewer crawler monitoring).

## Tasks / Subtasks

- [ ] Task 1: Crawler start/stop from app (AC: 1)
  - [ ] Expose start/stop (or run/abort) for the crawler via API (e.g. POST /api/v1/crawler/start, POST /api/v1/crawler/stop or equivalent).
  - [ ] Implement start: launch crawler with current config (mode, active DB path, rate limits, timeouts); ensure only one run at a time or define policy.
  - [ ] Implement stop: gracefully stop in-progress crawl (e.g. signal crawler to finish current item and exit).
  - [ ] Expose start/stop in web UI (reuse or extend crawler monitoring view from database-viewer).
- [ ] Task 2: View and edit crawler config in app (AC: 2)
  - [ ] Expose current crawler config (mode, DB path, rate limits, timeouts, etc.) via API (GET/PUT or equivalent) ([Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
  - [ ] Allow editing config in app (UI and/or API); persist changes so subsequent crawler runs use updated config.
  - [ ] Validate config (e.g. mode enum, positive timeouts, valid DB path from Story 6.2).
- [ ] Task 3: Align crawler config with active DB path (AC: 3)
  - [ ] Crawler config includes or references the active DB path from Story 6.2; crawler runs always write to the current active path ([Source: docs/prd/epic-6.md]).
  - [ ] When active DB is changed (6.2), crawler config used for next run reflects the new active path (auto or on next load).
- [ ] Task 4: Crawler status and logs in app (AC: 4)
  - [ ] Crawler status and logs remain visible in the app (build on existing database-viewer crawler status/logs API and UI) ([Source: docs/architecture/database-viewer-api-and-deployment.md]).
  - [ ] Ensure status reflects running/stopped and logs stream or refresh for active runs.
- [ ] Task 5: Tests and docs (AC: 1–4)
  - [ ] Add tests for start/stop, config read/write, and that crawler uses active path and new config ([Source: docs/architecture/testing-and-deployment-strategy.md]).
  - [ ] Document crawler control and config management in app docs.

## Dev Notes

### Context

Story 6.3 adds crawler control and config management to books-admin-app: start/stop from UI or API, view and edit crawler config (mode, DB path, rate limits, timeouts), and keep status/logs visible ([Source: docs/prd/epic-6.md]).

### Prerequisites

- Story 6.1: books-admin-app with merged crawler + viewer.
- Story 6.2: Active/backup DB versioning; active path is the single source for “where crawler writes” and “where data view reads.”

### Crawler config (existing)

- Config: mode (full, update, fix-download-all, full-database, test), DB path, rate limits, timeouts, logging; base merged with environments[NODE_ENV] ([Source: docs/architecture/crawler-pipeline-and-data-contract.md]). Crawler entry: `crawler/src/crawler.js`; config: `crawler/src/config/crawler-config.js`.

### Integration

- Crawler may run in-process or as subprocess (per 6.1). Start/stop must work with that design (e.g. spawn/kill subprocess or start/abort in-process run). Config passed to crawler on start must include active DB path from 6.2.

### File locations

- Within `books-admin-app/`: API routes for crawler start/stop and config GET/PUT; UI for crawler control and config form; reuse crawler status/logs endpoints and UI from merged viewer.

### Testing

- Unit tests for config validation and persistence. Integration: start crawler, verify it uses active path and config; stop; change config, start again, verify new config applied ([Source: docs/architecture/testing-and-deployment-strategy.md]).

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-16 | 1.0     | Story created from Epic 6      | Scrum Master |

## Dev Agent Record

### Agent Model Used

(To be filled by Dev Agent)

### Debug Log References

(To be filled by Dev Agent)

### Completion Notes List

(To be filled by Dev Agent)

### File List

(To be filled by Dev Agent)

## QA Results

(To be filled by QA Agent)

# Story 1.4: Data cleaning, validation, and schema

## Status

Draft

## Story

**As a** developer,  
**I want** crawled data to be cleaned, validated, and written against a fixed schema,  
**so that** the database-viewer and GrqaserApp can consume it without ad-hoc fixes.

## Acceptance Criteria

1. Text fields are cleaned (no HTML, normalized encoding).
2. Required fields and types are validated before write; invalid rows are logged/skipped.
3. Schema (tables, columns, constraints) is documented and versioned.
4. Data is written to the project’s SQLite database in the agreed location/path.

## Tasks / Subtasks

- [ ] Task 1: Text cleaning (AC: 1)
  - [ ] Clean all text fields: strip HTML, normalize encoding (e.g. UTF-8); apply before persist ([Source: docs/architecture/crawler-pipeline-and-data-contract.md], [Source: docs/architecture/coding-standards.md]).
  - [ ] Unit test cleaning logic (HTML removal, encoding).
- [ ] Task 2: Validation before write (AC: 2)
  - [ ] Validate required fields and types before each write; log and skip invalid rows ([Source: docs/architecture/data-models-and-schema.md], [Source: docs/architecture/coding-standards.md]).
  - [ ] Unit test validation; integration test that invalid rows are skipped and logged.
- [ ] Task 3: Schema documentation and versioning (AC: 3)
  - [ ] Document schema (tables, columns, constraints) in `docs/architecture/data-models-and-schema.md` or a versioned schema doc referenced there ([Source: docs/architecture/data-models-and-schema.md]).
  - [ ] Introduce schema version (e.g. version field or migration doc) so viewer and app can align.
- [ ] Task 4: Write to agreed DB location (AC: 4)
  - [ ] Ensure all writes go to the project’s SQLite database at the configured path (e.g. `crawler/data/grqaser.db` via `crawler-config.js`) ([Source: docs/architecture/source-tree.md], [Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
  - [ ] Integration test: data written to configured path; schema matches documentation.

## Dev Notes

### Previous Story Insights

Stories 1.1–1.3 deliver extraction, pagination, search, and relationships. This story formalizes cleaning, validation, schema docs, and single DB location so Phase 2 (database-viewer) can consume data without ad-hoc fixes.

### Data Models

- **Schema ownership:** Crawler defines and documents schema; same schema used by database-viewer and GrqaserApp ([Source: docs/architecture/data-models-and-schema.md]). Required fields and types must be validated before write; invalid rows logged/skipped.

### File Locations

- **Config (DB path):** `crawler/src/config/crawler-config.js`. **DB layer:** `crawler/src/models/database.js`. **DB file:** `crawler/data/grqaser.db`. **Schema doc:** `docs/architecture/data-models-and-schema.md` ([Source: docs/architecture/source-tree.md]).

### Technical Constraints

- No HTML in persisted text fields; validate required fields and types before write; config over hardcoding ([Source: docs/architecture/coding-standards.md]).

### Testing

- Unit tests for cleaning and validation; integration tests for DB writes and schema compliance ([Source: docs/architecture/testing-and-deployment-strategy.md]). Run: `npm test` from `crawler/`.

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 1     | Scrum Master |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

—

### Completion Notes List

—

### File List

—

## QA Results

—

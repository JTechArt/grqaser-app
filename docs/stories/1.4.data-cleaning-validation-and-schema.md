# Story 1.4: Data cleaning, validation, and schema

## Status

Done

## Story

**As a** developer,  
**I want** crawled data to be cleaned, validated, and written against a fixed schema,  
**so that** the database-viewer and GrqaserApp can consume it without ad-hoc fixes.

## Acceptance Criteria

1. Text fields are cleaned (no HTML, normalized encoding).
2. Required fields and types are validated before write; invalid rows are logged/skipped.
3. Schema (tables, columns, constraints) is documented and versioned.
4. Data is written to the project’s SQLite database in the agreed location/path.

## Tasks / Subtasks

- [x] Task 1: Text cleaning (AC: 1)
  - [x] Clean all text fields: strip HTML, normalize encoding (e.g. UTF-8); apply before persist ([Source: docs/architecture/crawler-pipeline-and-data-contract.md], [Source: docs/architecture/coding-standards.md]).
  - [x] Unit test cleaning logic (HTML removal, encoding).
- [x] Task 2: Validation before write (AC: 2)
  - [x] Validate required fields and types before each write; log and skip invalid rows ([Source: docs/architecture/data-models-and-schema.md], [Source: docs/architecture/coding-standards.md]).
  - [x] Unit test validation; integration test that invalid rows are skipped and logged.
- [x] Task 3: Schema documentation and versioning (AC: 3)
  - [x] Document schema (tables, columns, constraints) in `docs/architecture/data-models-and-schema.md` or a versioned schema doc referenced there ([Source: docs/architecture/data-models-and-schema.md]).
  - [x] Introduce schema version (e.g. version field or migration doc) so viewer and app can align.
- [x] Task 4: Write to agreed DB location (AC: 4)
  - [x] Ensure all writes go to the project’s SQLite database at the configured path (e.g. `crawler/data/grqaser.db` via `crawler-config.js`) ([Source: docs/architecture/source-tree.md], [Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
  - [x] Integration test: data written to configured path; schema matches documentation.

## Dev Notes

### Previous Story Insights

Stories 1.1–1.3 deliver extraction, pagination, search, and relationships. This story formalizes cleaning, validation, schema docs, and single DB location so Phase 2 (database-viewer) can consume data without ad-hoc fixes.

### Data Models

- **Schema ownership:** Crawler defines and documents schema; same schema used by database-viewer and GrqaserApp ([Source: docs/architecture/data-models-and-schema.md]). Required fields and types must be validated before write; invalid rows logged/skipped.

### File Locations

- **Config (DB path):** `crawler/src/config/crawler-config.js`. **DB layer:** `crawler/src/models/database.js`. **DB file:** `crawler/data/grqaser.db`. **Schema doc:** `docs/architecture/data-models-and-schema.md` ([Source: docs/architecture/source-tree.md]).

### Technical Constraints

- No HTML in persisted text fields; validate required fields and types before write; config over hardcoding ([Source: docs/architecture/coding-standards.md]).

### Testing

- Unit tests for cleaning and validation; integration tests for DB writes and schema compliance ([Source: docs/architecture/testing-and-deployment-strategy.md]). Run: `npm test` from `crawler/`.

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 1     | Scrum Master |
| 2025-02-14 | 1.1     | Story 1.4 implemented: cleaning/encoding test, validation integration test, schema version table and doc, DB path and schema match tests | Dev Agent |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

- `npm test` (crawler): 56 tests passed.

### Completion Notes List

- Task 1: Text cleaning already in place (Story 1.1). Added unit test for encoding (UTF-8 / non-ASCII) in text-cleaner.test.js.
- Task 2: Validation and skip/log already in crawler.saveBooks. Added integration test: validateBookRow + saveBook loop; assert invalid row not in DB and console.warn called.
- Task 3: Schema doc already in data-models-and-schema.md. Added schema version: doc states "Current schema version: 1"; database creates schema_version table with version=1 so viewer/app can align.
- Task 4: DB path from crawler-config.dbPath; Database uses config.dbPath. Added integration tests: Database uses configured dbPath when not overridden; schema_version exists with version 1; schema matches canonical DDL (BOOKS_TABLE_COLUMNS from books-table.js).

### File List

- crawler/src/utils/text-cleaner.js (unchanged; tests added)
- crawler/src/tests/text-cleaner.test.js (encoding unit test)
- crawler/src/tests/book-validator.test.js (unchanged)
- crawler/src/tests/database.integration.test.js (validation skip/log, schema match, dbPath, schema_version tests)
- crawler/src/schema/books-table.js (BOOKS_TABLE_COLUMNS export)
- crawler/src/models/database.js (createSchemaVersionTable)
- docs/architecture/data-models-and-schema.md (schema version 1 and schema_version table note)

—

## QA Results

### Review Date: 2025-02-14

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Story 1.4 formalizes cleaning, validation, schema versioning, and DB location as required for Phase 2. Text cleaning is already in place (Story 1.1); encoding test added (UTF-8 / non-ASCII preserved). Validation before write is in crawler.saveBooks (validateBookRow); integration test confirms invalid row skipped, not persisted, and console.warn called. Schema is documented in data-models-and-schema.md with "Current schema version: 1" and schema_version table; Database.createSchemaVersionTable() creates the table and inserts version=1 so viewer/app can align. BOOKS_TABLE_COLUMNS exported from books-table.js for schema compliance checks; integration test asserts books table columns match canonical DDL. DB path comes from crawler-config.dbPath; Database uses config.dbPath (integration test asserts when not overridden). Implementation matches Dev Notes and architecture.

### Refactoring Performed

None. No blocking issues warranted code changes during review.

### Compliance Check

- Coding Standards: ✓ No HTML in text; validate before write; config for DB path.
- Project Structure: ✓ Schema version in database.js; BOOKS_TABLE_COLUMNS in books-table.js; doc updated.
- Testing Strategy: ✓ Unit test for encoding; integration tests for validation skip/log, schema match, dbPath, schema_version.
- All ACs Met: ✓ AC1 text cleaning (and encoding test); AC2 validation and skip/log; AC3 schema documented and versioned; AC4 writes to configured path.

### Improvements Checklist

- [x] Requirements traceability verified (all four ACs have tests or doc).
- [x] Test run confirmed (56 tests passed).

### Security Review

No new auth or input surface. DB path is config-driven; schema version table is read-only for consumers.

### Performance Considerations

Schema version table is created once; single row. Schema match test uses PRAGMA table_info; acceptable.

### Files Modified During Review

None.

### Gate Status

Gate: PASS → docs/qa/gates/1.4-data-cleaning-validation-and-schema.yml

### Recommended Status

✓ Ready for Done

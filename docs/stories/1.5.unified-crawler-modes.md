# Story 1.5: Unified crawler with configurable modes

## Status

Done

## Story

**As an** operator,  
**I want** a single crawler entrypoint that can run in different modes (full discovery, update existing, fix download-all, full-database update, optional test),  
**so that** we maintain one codebase, one config, and one schema while supporting discovery, repair, and bulk-update workflows.

## Context

There are five crawler scripts (see `learning/crawler-comparison-and-merge.md`). The main production crawler is `crawler.js` (queue-based discovery + detail). The others (`improved-crawler.js`, `full-database-crawler.js`, `fix-download-all-crawler.js`) re-crawl existing books and UPDATE them (e.g. to fix download-all URLs, populate chapter_urls, or refresh metadata). `test-crawler.js` is a lightweight dev harness that runs extraction against a small sample and writes to a test DB. This story merges the update/repair behaviors into the main crawler via a **mode** and removes the redundant scripts.

## Acceptance Criteria

1. **Modes**  
   The crawler supports a configured or CLI mode:
   - `full` (default): current behavior — URL queue, listing + detail, INSERT OR REPLACE; target books and limits from config.
   - `update`: select existing books that need refresh (e.g. no/missing audio, Unknown Author, no cover); re-crawl detail; UPDATE by id; optional limit.
   - `fix-download-all`: select only books with download-all or missing chapter_urls; re-crawl and UPDATE with individual MP3 URLs and chapter_urls.
   - `full-database`: select books with download-all OR missing chapter_urls OR crawl_status ≠ completed; re-crawl and UPDATE; optional concurrency (e.g. multiple Puppeteer pages).
   - `test` (optional): limit N books from DB, run detail extraction, write to a test DB or test table (no change to main books).

2. **Single config and schema**  
   All modes use `crawler-config.js` (and env overrides). DB path is config-driven; test mode may override output DB/table. Books schema includes `chapter_urls` where applicable; main crawler read/write path supports it for update modes.

3. **Shared extraction and validation**  
   All modes use the same detail extraction (title, author, description, duration, cover, main_audio_url, download_url, chapter_urls, has_chapters, chapter_count) and the same validation/cleaning (validateBookRow, validateAudioUrl, cleanText, normalizeAuthor, normalizeCategory, normalizeDurationForStorage).

4. **Observability**  
   All modes write to crawl_logs (and optional file logging) so progress and errors are visible in one place.

5. **Removal of redundant crawlers**  
   After merge and verification: remove `improved-crawler.js`, `full-database-crawler.js`, and `fix-download-all-crawler.js`. Optionally keep `test-crawler.js` as a thin wrapper that invokes the main crawler in `test` mode, or document it as deprecated in favor of `--mode=test`.

## Tasks / Subtasks

- [x] Task 1: Add mode configuration and entrypoint (AC: 1, 2)
  - [x] Add `mode` to crawler-config (e.g. `full` | `update` | `fix-download-all` | `full-database` | `test`) and document; support override via env or CLI (e.g. `CRAWLER_MODE=update` or `--mode=update`).
  - [x] In main crawler, branch on mode: `full` runs existing queue-based crawl; other modes run the corresponding “select books from DB → re-crawl detail → persist” flow.
  - [x] For `test` mode: add config/CLI for limit N and output DB path (or test table); ensure no writes to main books.
- [x] Task 2: Implement update / fix-download-all / full-database selection and UPDATE path (AC: 1, 2, 3)
  - [x] Implement book selection queries for `update` (e.g. no audio, Unknown Author, no cover), `fix-download-all` (download-all or missing chapter_urls), `full-database` (download-all OR no chapter_urls OR crawl_status ≠ completed).
  - [x] Reuse or extend existing `extractBookDetail()` (and shared validation/cleaning) so all modes produce the same normalized book shape; ensure chapter_urls and individual MP3 extraction logic from improved/fix-download-all/full-database are merged into this single implementation.
  - [x] Add UPDATE-by-id path in main crawler (in addition to existing INSERT OR REPLACE) and use it for update/fix-download-all/full-database modes; ensure chapter_urls is written when schema supports it.
- [x] Task 3: Schema and config for chapter_urls and test mode (AC: 2)
  - [x] If not already present, add `chapter_urls` to canonical books schema (e.g. `crawler/src/schema/books-table.js`) and to save/update SQL in crawler.js; align with database-viewer if it already has this column.
  - [x] Ensure config supports dbPath (and optional testDbPath / test table for test mode); single default DB path (e.g. crawler/data/grqaser.db); document when to use database-viewer path if needed.
- [x] Task 4: Concurrency and logging for update modes (AC: 1, 4)
  - [x] Add optional multi-page concurrency for `full-database` (and optionally `update`/`fix-download-all`) via config (e.g. maxConcurrentPages); default 1 to preserve current behavior.
  - [x] Ensure all modes call the same logging (crawl_logs + optional file) so runs are visible in one place.
- [x] Task 5: Remove redundant crawlers and update docs (AC: 5)
  - [x] Remove `improved-crawler.js`, `full-database-crawler.js`, `fix-download-all-crawler.js` after verification.
  - [x] Either make `test-crawler.js` a thin wrapper that runs main crawler with `--mode=test` and test DB, or mark it deprecated and document `--mode=test` in README/crawler docs.
  - [x] Update `learning/crawler-file-map.md` and `learning/crawler-comparison-and-merge.md` to state that a single crawler with modes is the source of truth; add a short “How to run” (full / update / fix-download-all / full-database / test) to crawler README.

## Dev Notes

### Reference

- **Comparison and merge plan:** `learning/crawler-comparison-and-merge.md` (feature table, recommendations, merge list).
- **Crawler file map:** `learning/crawler-file-map.md`.

### Data Models

- **Books:** Include `chapter_urls` (e.g. TEXT/JSON array) in schema and in crawler save/update path for modes that persist chapter URLs ([Source: learning/crawler-comparison-and-merge.md]). Other columns already in use: id, title, author, description, duration, duration_formatted, type, language, category, rating, cover_image_url, main_audio_url, download_url, crawl_status, has_chapters, chapter_count, updated_at.

### File Locations

- **Main crawler:** `crawler/src/crawler.js`.
- **Config:** `crawler/src/config/crawler-config.js`.
- **Schema:** `crawler/src/schema/books-table.js`.
- **Utils:** `crawler/src/utils/book-validator.js`, `url-validator.js`, `text-cleaner.js`, `duration-parser.js`.
- **To remove after merge:** `crawler/src/improved-crawler.js`, `crawler/src/full-database-crawler.js`, `crawler/src/fix-download-all-crawler.js`.

### Technical Constraints

- Node.js LTS, Puppeteer, SQLite; no HTML in text fields; validate before write ([Source: docs/architecture/coding-standards.md]). Reuse existing validation and cleaning in all modes.

### Testing

- Unit tests for mode selection and book-selection queries; integration test that each mode runs without error and (where applicable) updates or inserts as expected. Optional: smoke test that fix-download-all and full-database produce chapter_urls. Run: `npm test` from `crawler/`.

## Dev Agent Record

### File List

- **Modified:** `crawler/src/config/crawler-config.js` — mode, testLimit, testDbPath, updateLimit, maxConcurrentPages; env overrides.
- **Modified:** `crawler/src/crawler.js` — parseCli, mode branching, runUpdateMode/runUpdateModeInternal, getBooksForMode, updateBookById, ensureChapterUrlsColumn; extractBookDetail chapter_urls and optional page; launchBrowser multi-page; saveBookToDatabase chapter_urls; exports parseCli, VALID_MODES.
- **Modified:** `crawler/src/schema/books-table.js` — chapter_urls column and BOOKS_TABLE_COLUMNS.
- **Modified:** `crawler/README.md` — How to run (modes table), CLI/env, project structure, config.
- **Modified:** `learning/crawler-file-map.md` — single entrypoint; test-crawler as thin wrapper.
- **Modified:** `learning/crawler-comparison-and-merge.md` — status: Story 1.5 done, single crawler source of truth.
- **Added:** `crawler/src/tests/crawler-modes.test.js` — unit tests for parseCli, constructor overrides, getBooksForMode.
- **Replaced:** `crawler/src/test-crawler.js` — thin wrapper spawning main crawler with --mode=test.
- **Deleted:** `crawler/src/improved-crawler.js`, `crawler/src/full-database-crawler.js`, `crawler/src/fix-download-all-crawler.js`.

### Completion Notes

- Mode and CLI/env wired in config and crawler; full mode unchanged; update/fix-download-all/full-database/test run select → re-crawl detail → persist (UPDATE or test DB).
- Book selection queries and UPDATE-by-id implemented; extractBookDetail extended with chapter_urls and broader MP3 extraction; ensureChapterUrlsColumn for existing DBs.
- Optional multi-page concurrency for update modes via maxConcurrentPages; all modes use same crawl_logs and file logging.
- Redundant crawlers removed; test-crawler is thin wrapper; README and learning docs updated.

### Change Log

| Date       | Version | Description                | Author |
|-----------|---------|----------------------------|--------|
| 2025-02-15 | 1.0     | Story created (unified crawler modes) | —      |
| 2025-02-15 | 1.1     | Story 1.5 implemented (unified modes, chapter_urls, docs) | Dev    |

## QA Results

### Review Date: 2025-02-15

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Story 1.5 is implemented as specified. Mode is config- and CLI-driven (mode, testLimit, testDbPath, updateLimit, maxConcurrentPages; parseCli, env CRAWLER_*). Main crawler branches on mode: full runs existing queue-based crawl; update, fix-download-all, full-database, and test run getBooksForMode → re-crawl detail → persist (UPDATE by id or test DB). Book selection queries match AC (update: no audio / Unknown Author / no cover; fix-download-all: download-all or missing chapter_urls; full-database: download-all OR no chapter_urls OR crawl_status ≠ completed; test: limit N). extractBookDetail includes chapter_urls and MP3 extraction; ensureChapterUrlsColumn migrates existing DBs. saveBookToDatabase and updateBookById write chapter_urls (JSON). Schema has chapter_urls in books-table.js and BOOKS_TABLE_COLUMNS. test-crawler.js is a thin wrapper (spawns main crawler with --mode=test). improved-crawler.js, full-database-crawler.js, fix-download-all-crawler.js are removed. README and learning docs (crawler-file-map.md, crawler-comparison-and-merge.md) updated. All modes use same crawl_logs and optional file logging; optional maxConcurrentPages for update modes.

### Refactoring Performed

None during review.

### Compliance Check

- Coding Standards: ✓ Single config and schema; shared validation and cleaning; config over hardcoding.
- Project Structure: ✓ Single entrypoint; schema and config as specified.
- Testing Strategy: ✓ crawler-modes.test.js covers parseCli, constructor overrides, getBooksForMode (update and test).
- All ACs Met: ✓ AC1–AC5 implemented as described.

### Improvements Checklist

- [x] Requirements traceability verified (modes, config, schema, shared extraction, observability, removal of redundant crawlers).
- [x] Story 1.5–specific tests (crawler-modes.test.js) pass.
- [x] Fix failing book-validator tests (validations implemented: empty main_audio_url, duration ≥ 0, rating 0–5, rating_count ≥ 0, language ≤ 10 chars).

### Security Review

No new auth or input surface. Mode and limits are config/CLI; test DB path is configurable.

### Performance Considerations

maxConcurrentPages allows parallel detail fetches for update modes; default 1 preserves prior behavior.

### Files Modified During Review

None.

### Gate Status

Gate: CONCERNS → docs/qa/gates/1.5-unified-crawler-modes.yml

### Recommended Status

✗ Changes Required — fix book-validator.test.js failures (implement validations or adjust tests) so full suite passes, then re-run QA or set Ready for Done.

---

### Review Date: 2025-02-15 (re-review after dev fixes)

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Dev implemented the book-validator recommendations. `validateBookRow` now rejects: empty/whitespace main_audio_url; negative or non-numeric duration; rating outside 0–5; non-integer or negative rating_count; language longer than 10 characters. All 70 tests pass.

### Refactoring Performed

None during re-review.

### Compliance Check

- Coding Standards: ✓ (unchanged)
- Project Structure: ✓ (unchanged)
- Testing Strategy: ✓ 70 tests passed (including book-validator and crawler-modes).
- All ACs Met: ✓ (unchanged)

### Improvements Checklist

- [x] Requirements traceability verified (unchanged).
- [x] Story 1.5–specific tests pass (unchanged).
- [x] Book-validator validations implemented; full suite passes.

### Security Review

No change.

### Performance Considerations

No change.

### Files Modified During Review

None.

### Gate Status

Gate: PASS → docs/qa/gates/1.5-unified-crawler-modes.yml

### Recommended Status

✓ Ready for Done

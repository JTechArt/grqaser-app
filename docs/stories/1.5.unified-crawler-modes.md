# Story 1.5: Unified crawler with configurable modes

## Status

Draft

## Story

**As an** operator,  
**I want** a single crawler entrypoint that can run in different modes (full discovery, update existing, fix download-all, full-database update, optional test),  
**so that** we maintain one codebase, one config, and one schema while supporting discovery, repair, and bulk-update workflows.

## Context

There are five crawler scripts (see `learning/crawler-comparison-and-merge.md`). The main production crawler is `crawler.js` (queue-based discovery + detail). The others (`improved-crawler.js`, `full-database-crawler.js`, `fix-download-all-crawler.js`) re-crawl existing books and UPDATE them (e.g. to fix download-all URLs, populate chapter_urls, or refresh metadata). `test-crawler.js` is a lightweight dev harness that runs extraction against a small sample and writes to a test DB. This story merges the update/repair behaviors into the main crawler via a **mode** and removes the redundant scripts.

## Acceptance Criteria

1. **Modes**  
   The crawler supports a configured or CLI mode:
   - `full` (default): current behavior — URL queue, listing + detail, INSERT OR REPLACE; target books and limits from config.
   - `update`: select existing books that need refresh (e.g. no/missing audio, Unknown Author, no cover); re-crawl detail; UPDATE by id; optional limit.
   - `fix-download-all`: select only books with download-all or missing chapter_urls; re-crawl and UPDATE with individual MP3 URLs and chapter_urls.
   - `full-database`: select books with download-all OR missing chapter_urls OR crawl_status ≠ completed; re-crawl and UPDATE; optional concurrency (e.g. multiple Puppeteer pages).
   - `test` (optional): limit N books from DB, run detail extraction, write to a test DB or test table (no change to main books).

2. **Single config and schema**  
   All modes use `crawler-config.js` (and env overrides). DB path is config-driven; test mode may override output DB/table. Books schema includes `chapter_urls` where applicable; main crawler read/write path supports it for update modes.

3. **Shared extraction and validation**  
   All modes use the same detail extraction (title, author, description, duration, cover, main_audio_url, download_url, chapter_urls, has_chapters, chapter_count) and the same validation/cleaning (validateBookRow, validateAudioUrl, cleanText, normalizeAuthor, normalizeCategory, normalizeDurationForStorage).

4. **Observability**  
   All modes write to crawl_logs (and optional file logging) so progress and errors are visible in one place.

5. **Removal of redundant crawlers**  
   After merge and verification: remove `improved-crawler.js`, `full-database-crawler.js`, and `fix-download-all-crawler.js`. Optionally keep `test-crawler.js` as a thin wrapper that invokes the main crawler in `test` mode, or document it as deprecated in favor of `--mode=test`.

## Tasks / Subtasks

- [ ] Task 1: Add mode configuration and entrypoint (AC: 1, 2)
  - [ ] Add `mode` to crawler-config (e.g. `full` | `update` | `fix-download-all` | `full-database` | `test`) and document; support override via env or CLI (e.g. `CRAWLER_MODE=update` or `--mode=update`).
  - [ ] In main crawler, branch on mode: `full` runs existing queue-based crawl; other modes run the corresponding “select books from DB → re-crawl detail → persist” flow.
  - [ ] For `test` mode: add config/CLI for limit N and output DB path (or test table); ensure no writes to main books.
- [ ] Task 2: Implement update / fix-download-all / full-database selection and UPDATE path (AC: 1, 2, 3)
  - [ ] Implement book selection queries for `update` (e.g. no audio, Unknown Author, no cover), `fix-download-all` (download-all or missing chapter_urls), `full-database` (download-all OR no chapter_urls OR crawl_status ≠ completed).
  - [ ] Reuse or extend existing `extractBookDetail()` (and shared validation/cleaning) so all modes produce the same normalized book shape; ensure chapter_urls and individual MP3 extraction logic from improved/fix-download-all/full-database are merged into this single implementation.
  - [ ] Add UPDATE-by-id path in main crawler (in addition to existing INSERT OR REPLACE) and use it for update/fix-download-all/full-database modes; ensure chapter_urls is written when schema supports it.
- [ ] Task 3: Schema and config for chapter_urls and test mode (AC: 2)
  - [ ] If not already present, add `chapter_urls` to canonical books schema (e.g. `crawler/src/schema/books-table.js`) and to save/update SQL in crawler.js; align with database-viewer if it already has this column.
  - [ ] Ensure config supports dbPath (and optional testDbPath / test table for test mode); single default DB path (e.g. crawler/data/grqaser.db); document when to use database-viewer path if needed.
- [ ] Task 4: Concurrency and logging for update modes (AC: 1, 4)
  - [ ] Add optional multi-page concurrency for `full-database` (and optionally `update`/`fix-download-all`) via config (e.g. maxConcurrentPages); default 1 to preserve current behavior.
  - [ ] Ensure all modes call the same logging (crawl_logs + optional file) so runs are visible in one place.
- [ ] Task 5: Remove redundant crawlers and update docs (AC: 5)
  - [ ] Remove `improved-crawler.js`, `full-database-crawler.js`, `fix-download-all-crawler.js` after verification.
  - [ ] Either make `test-crawler.js` a thin wrapper that runs main crawler with `--mode=test` and test DB, or mark it deprecated and document `--mode=test` in README/crawler docs.
  - [ ] Update `learning/crawler-file-map.md` and `learning/crawler-comparison-and-merge.md` to state that a single crawler with modes is the source of truth; add a short “How to run” (full / update / fix-download-all / full-database / test) to crawler README.

## Dev Notes

### Reference

- **Comparison and merge plan:** `learning/crawler-comparison-and-merge.md` (feature table, recommendations, merge list).
- **Crawler file map:** `learning/crawler-file-map.md`.

### Data Models

- **Books:** Include `chapter_urls` (e.g. TEXT/JSON array) in schema and in crawler save/update path for modes that persist chapter URLs ([Source: learning/crawler-comparison-and-merge.md]). Other columns already in use: id, title, author, description, duration, duration_formatted, type, language, category, rating, cover_image_url, main_audio_url, download_url, crawl_status, has_chapters, chapter_count, updated_at.

### File Locations

- **Main crawler:** `crawler/src/crawler.js`.
- **Config:** `crawler/src/config/crawler-config.js`.
- **Schema:** `crawler/src/schema/books-table.js`.
- **Utils:** `crawler/src/utils/book-validator.js`, `url-validator.js`, `text-cleaner.js`, `duration-parser.js`.
- **To remove after merge:** `crawler/src/improved-crawler.js`, `crawler/src/full-database-crawler.js`, `crawler/src/fix-download-all-crawler.js`.

### Technical Constraints

- Node.js LTS, Puppeteer, SQLite; no HTML in text fields; validate before write ([Source: docs/architecture/coding-standards.md]). Reuse existing validation and cleaning in all modes.

### Testing

- Unit tests for mode selection and book-selection queries; integration test that each mode runs without error and (where applicable) updates or inserts as expected. Optional: smoke test that fix-download-all and full-database produce chapter_urls. Run: `npm test` from `crawler/`.

## Change Log

| Date       | Version | Description                | Author |
|-----------|---------|----------------------------|--------|
| 2025-02-15 | 1.0     | Story created (unified crawler modes) | —      |

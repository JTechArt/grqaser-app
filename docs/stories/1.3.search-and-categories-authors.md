# Story 1.3: Search and categories/authors

## Status

Draft

## Story

**As an** operator,  
**I want** the crawler to use site search and category/author structures where available,  
**so that** catalog and relationships (book–author, book–category) are complete and consistent.

## Acceptance Criteria

1. Search flow on the source site works; results are merged into the catalog without duplicates.
2. Categories and authors are extracted and linked to books.
3. Data relationships support filtering and stats in the database-viewer and GrqaserApp.

## Tasks / Subtasks

- [ ] Task 1: Site search and merge (AC: 1)
  - [ ] Implement search flow on the source site (grqaser.org); execute search and collect result URLs.
  - [ ] Merge search results into the catalog; detect and avoid duplicates (same book ID/URL).
  - [ ] Unit or integration test: search yields results; merge does not create duplicates.
- [ ] Task 2: Categories and authors extraction (AC: 2)
  - [ ] Extract categories and authors from listing/detail pages; link to books (columns or related tables per [Source: docs/architecture/data-models-and-schema.md]).
  - [ ] Normalize category and author names where needed; ensure referential consistency.
  - [ ] Persist book–category and book–author relationships for filtering and stats.
- [ ] Task 3: Relationships for viewer and app (AC: 3)
  - [ ] Ensure schema supports filtering by category/author and stats (counts/aggregates) as required by database-viewer and GrqaserApp ([Source: docs/architecture/data-models-and-schema.md]).
  - [ ] Integration test: relationships are queryable (e.g. books by category, author counts).

## Dev Notes

### Previous Story Insights

Stories 1.1 and 1.2 provide metadata, audio URLs, full catalog traversal, pagination, and deduplication. This story adds search as a discovery path and formalizes categories/authors and relationships.

### Data Models

- **Books** with category and author links (columns or normalized tables). Relationships must support filtering and stats ([Source: docs/architecture/data-models-and-schema.md]). Align with Books key attributes (category, author, etc.) and any separate category/author tables if introduced.

### File Locations

- **Crawler entry:** `crawler/src/crawler.js`. **Config:** `crawler/src/config/crawler-config.js`. **DB:** `crawler/src/models/database.js` (books + any category/author tables). **URL queue:** `crawler/src/utils/url-queue-manager.js` ([Source: docs/architecture/crawler-pipeline-and-data-contract.md]).

### Technical Constraints

- Same as Epic 1: Node.js, Puppeteer, SQLite; no HTML in text fields; validate before write ([Source: docs/architecture/coding-standards.md]).

### Testing

- Unit tests for search parsing and merge/dedup logic; integration tests for relationship persistence and queryability ([Source: docs/architecture/testing-and-deployment-strategy.md]). Run: `npm test` from `crawler/`.

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 1     | Scrum Master |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

—

### Completion Notes List

—

### File List

—

## QA Results

—

# Story 1.2: Full catalog and pagination

## Status

Draft

## Story

**As an** operator,  
**I want** the crawler to traverse all listing pages and book detail pages (pagination),  
**so that** the full catalog (target 950+ books) is present in the database.

## Acceptance Criteria

1. Listing and detail pagination are implemented with configurable rate limiting.
2. All target books are crawled; duplicates are detected and avoided.
3. Each book has a complete metadata set and audio URLs where available.
4. Crawler run progress and errors are loggable and (optionally) visible via database-viewer.

## Tasks / Subtasks

- [ ] Task 1: Pagination and rate limiting (AC: 1)
  - [ ] Implement listing-page pagination (traverse all listing pages) and detail-page flow.
  - [ ] Add configurable rate limiting (delays, concurrency) in `crawler/src/config/crawler-config.js` ([Source: docs/architecture/crawler-pipeline-and-data-contract.md]).
  - [ ] Unit or integration test that pagination and rate limits are applied.
- [ ] Task 2: Full catalog and deduplication (AC: 2, 3)
  - [ ] Ensure all target books (950+ per PRD) are discovered and crawled; detect and avoid duplicates (e.g. by unique ID or URL).
  - [ ] Persist complete metadata and audio URLs per book (builds on Story 1.1 schema).
  - [ ] Integration test: crawl produces expected volume; no duplicate books.
- [ ] Task 3: Progress and error logging (AC: 4)
  - [ ] Log crawler run progress and errors (optional file logging under `crawler/logs/` ([Source: docs/architecture/tech-stack.md]).
  - [ ] Ensure logs are structured so they can later be exposed via database-viewer (e.g. crawl_logs table or log format).
  - [ ] Test that progress and errors are loggable.

## Dev Notes

### Previous Story Insights

Story 1.1 delivers normalized book metadata, duration, audio URLs, categories/ratings/descriptions/language, and schema documentation. This story assumes that schema and extraction logic exist; focus on traversal, pagination, rate limiting, deduplication, and logging.

### Data Models

- **Books:** Same as Story 1.1; full metadata and audio URLs per book. **URL queue:** `crawler/src/utils/url-queue-manager.js`; url_type, priority, status, retry_count ([Source: docs/architecture/data-models-and-schema.md]). **Crawl logs:** level, message, book_id, url, error_details for progress/errors ([Source: docs/architecture/data-models-and-schema.md]).

### File Locations

- **Crawler entry:** `crawler/src/crawler.js`. **Config:** `crawler/src/config/crawler-config.js` (rate limit, delays, DB path, logging). **DB:** `crawler/src/models/database.js`. **URL queue:** `crawler/src/utils/url-queue-manager.js`. **DB file:** `crawler/data/grqaser.db`. **Logs:** `crawler/logs/` (optional) ([Source: docs/architecture/source-tree.md], [Source: docs/architecture/crawler-pipeline-and-data-contract.md]).

### Technical Constraints

- Node.js, Puppeteer, SQLite; timeouts and retries (e.g. 30s, 3 retries); config over hardcoding ([Source: docs/architecture/tech-stack.md], [Source: docs/architecture/coding-standards.md]).

### Testing

- Unit tests for pagination/rate-limit logic where isolated; integration tests for DB writes and full-catalog/dedup behavior ([Source: docs/architecture/testing-and-deployment-strategy.md]). Run: `npm test` from `crawler/`.

## Change Log

| Date       | Version | Description                    | Author       |
|-----------|---------|--------------------------------|--------------|
| 2025-02-14 | 1.0     | Story created from Epic 1     | Scrum Master |

## Dev Agent Record

### Agent Model Used

—

### Debug Log References

—

### Completion Notes List

—

### File List

—

## QA Results

—
